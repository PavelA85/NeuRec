{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Short Introduction to NeuRec\n",
    "\n",
    "This example aims to describe the building blocks of NeuRec.\n",
    "\n",
    "Following this example, researchers can fast implement their idea and conduct experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "- numpy>=1.17\n",
    "- scipy>=1.3.1\n",
    "- pandas>=0.17\n",
    "- reckit==0.2.0\n",
    "- tensorflow==1.14.0 or pytorch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurator\n",
    "\n",
    "Read configuration\n",
    "\n",
    "The class `Configurator` is designed to read arguments from ini-style configuration files and/or parse arguments from command line.\n",
    "The arguments can convert to `int`, `float`, `bool`, `list` and `None` automatically.\n",
    "\n",
    "The format of arguments in command line is \"--arg_name arg_value\":\n",
    "```bash\n",
    "python main.py --model MF --num_thread 8 --metric [\"Recall\", \"NDCG\"]\n",
    "```\n",
    "\n",
    "Using `Configurator.add_config()` to read ini-style configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reckit import Configurator\n",
    "\n",
    "config = Configurator()\n",
    "config.add_config(\"Preprocess.ini\", section=\"Preprocess\")\n",
    "# config.parse_cmd()  # Parse the arguments from command line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `section` will be activated only if there are more than one sections in configuration file, i.e. if there is only one section and whatever the name is, the arguments will be read from it.\n",
    "\n",
    "**Note**, the arguments from command line have the highest priority than that from configuration file.\n",
    "That is, if there are same argument name in configuration file and command line, the value in the former will be overwritten by that in the latter, whenever the command line is phased before or after reading ini files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor\n",
    "\n",
    "This process is not necessary.\n",
    "You can ignore this step if your dataset has already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "filtering items...\n",
      "filtering users...\n",
      "remapping user IDs...\n",
      "remapping item IDs...\n",
      "splitting data by ratio...\n",
      "saving data to disk...\n",
      "2020-11-01 07:33:47.051: \n",
      "columns = UIRT\n",
      "filename = dataset/ml-100k.rating\n",
      "sep = \t\n",
      "item_min = 5\n",
      "user_min = 5\n",
      "remap_user_id = True\n",
      "remap_item_id = True\n",
      "split_by = ratio\n",
      "train = 0.7\n",
      "valid = 0.0\n",
      "test = 0.3\n",
      "by_time = False\n",
      "2020-11-01 07:33:47.053: Data statistic:\n",
      "2020-11-01 07:33:47.054: The number of users: 943\n",
      "2020-11-01 07:33:47.056: The number of items: 1349\n",
      "2020-11-01 07:33:47.058: The number of ratings: 99287\n",
      "2020-11-01 07:33:47.060: Average actions of users: 105.29\n",
      "2020-11-01 07:33:47.063: Average actions of items: 73.60\n",
      "2020-11-01 07:33:47.065: The sparsity of the dataset: 92.195075%\n"
     ]
    }
   ],
   "source": [
    "from reckit import Preprocessor\n",
    "\n",
    "data = Preprocessor()\n",
    "data.load_data(config.filename, sep=config.separator, columns=config.file_column)\n",
    "if config.drop_duplicates is True:\n",
    "    data.drop_duplicates(keep=\"first\")  # drop duplicates except for the first or last occurrence\n",
    "\n",
    "data.filter_data(user_min=5, item_min=5)  # filter users and items with a few interactions\n",
    "if config.remap_id is True:\n",
    "    data.remap_data_id()  # convert user and item IDs to integers, start from 0\n",
    "\n",
    "if config.splitter == \"leave_out\":\n",
    "    data.split_data_by_leave_out(valid=config.valid, test=config.test,\n",
    "                                 by_time=config.by_time)\n",
    "elif config.splitter == \"ratio\":\n",
    "    data.split_data_by_ratio(train=config.train, valid=config.valid,\n",
    "                             test=config.test, by_time=config.by_time)\n",
    "\n",
    "data.save_data()  # save the preprocessed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = Configurator()\n",
    "config.add_config(\"NeuRec.ini\", section=\"NeuRec\")  # read basic settings\n",
    "# config.parse_cmd()\n",
    "\n",
    "model_cfg = os.path.join(\"conf\", \"MF.ini\")  # model cfg path\n",
    "config.add_config(model_cfg, section=\"hyperparameters\", used_as_summary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefix name of data files is same as the data_dir, and the suffix/extension names are 'train', 'test', 'user2id', 'item2id'.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "    data_dir\n",
    "        ├── data_dir.train      // training data\n",
    "        ├── data_dir.valid      // validation data, optional\n",
    "        ├── data_dir.test       // test data\n",
    "        ├── data_dir.user2id    // user to id, optional\n",
    "        ├── data_dir.item2id    // item to id, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics:\n",
      "Name: ml-100k_ratio_u5_i5\n",
      "The number of users: 943\n",
      "The number of items: 1349\n",
      "The number of ratings: 99287\n",
      "Average actions of users: 105.29\n",
      "Average actions of items: 73.60\n",
      "The sparsity of the dataset: 92.195075%\n",
      "\n",
      "The number of training: 69918\n",
      "The number of validation: 0\n",
      "The number of testing: 29369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\NeuRec\\data\\dataset.py:264: UserWarning: dataset\\ml-100k_ratio_u5_i5\\ml-100k_ratio_u5_i5.valid does not exist.\n",
      "  warnings.warn(\"%s does not exist.\" % valid_file)\n"
     ]
    }
   ],
   "source": [
    "from data import Dataset\n",
    "dataset = Dataset(config.data_dir, config.sep, config.file_column)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger\n",
    "\n",
    "This class can show a message on standard output and write it into the file named `filename` simultaneously.\n",
    "This is convenient for observing and saving training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-01 07:33:47.358: Dataset statistics:\n",
      "Name: ml-100k_ratio_u5_i5\n",
      "The number of users: 943\n",
      "The number of items: 1349\n",
      "The number of ratings: 99287\n",
      "Average actions of users: 105.29\n",
      "Average actions of items: 73.60\n",
      "The sparsity of the dataset: 92.195075%\n",
      "\n",
      "The number of training: 69918\n",
      "The number of validation: 0\n",
      "The number of testing: 29369\n",
      "2020-11-01 07:33:47.363: NeuRec:[NeuRec]:\n",
      "recommender=MF\n",
      "platform=tensorflow\n",
      "data_dir='dataset\\ml-100k_ratio_u5_i5'\n",
      "file_column=UIRT\n",
      "sep='\\t'\n",
      "gpu_id=0\n",
      "gpu_mem=0.99\n",
      "metric=[\"Precision\", \"Recall\", \"MAP\", \"NDCG\", \"MRR\"]\n",
      "top_k=[10,20]\n",
      "test_thread=4\n",
      "test_batch_size=64\n",
      "seed=2020\n",
      "\n",
      "MF:[hyperparameters]:\n",
      "lr=0.001\n",
      "reg=0.001\n",
      "embedding_size=64\n",
      "batch_size=1024\n",
      "epochs=5\n",
      "is_pairwise=True\n",
      "loss_func=bpr\n",
      "param_init=normal\n"
     ]
    }
   ],
   "source": [
    "from reckit import Logger\n",
    "import time\n",
    "# create logger filename\n",
    "data_name = dataset.data_name  # dataset name\n",
    "timestamp = time.time()  # run time\n",
    "model_name = config.recommender  # model name\n",
    "model_param = config.summarize()  # return a string of model parameters\n",
    "param_str = f\"{data_name}_{model_name}_{model_param}\"\n",
    "run_id = f\"{param_str[:150]}_{timestamp:.8f}\"\n",
    "\n",
    "log_dir = os.path.join(\"log\", data_name, model_name)  # logger directory\n",
    "logger_name = os.path.join(log_dir, run_id + \".log\")  # full path\n",
    "logger = Logger(logger_name)\n",
    "\n",
    "logger.info(dataset)  # show and write dataset info\n",
    "logger.info(config)  # show and write config info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "Evaluation metrics of `Evaluator` are configurable and can automatically fit both leave-one-out and fold-out data splitting without specific indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reckit import Evaluator\n",
    "\n",
    "user_train_dict = dataset.train_data.to_user_dict()\n",
    "user_test_dict = dataset.test_data.to_user_dict()\n",
    "evaluator = Evaluator(user_train_dict, user_test_dict,\n",
    "                      metric=config.metric, top_k=config.top_k,\n",
    "                      batch_size=config.test_batch_size,\n",
    "                      num_thread=config.test_thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairwiseSampler\n",
    "\n",
    "`PairwiseSampler` is an encapsulation of `Dataset` to do negative item sampling and construct training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import PairwiseSampler\n",
    "\n",
    "data_iter = PairwiseSampler(dataset.train_data, num_neg=1,\n",
    "                            batch_size=config[\"batch_size\"], \n",
    "                            shuffle=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from util.pytorch import inner_product\n",
    "from util.pytorch import get_initializer\n",
    "\n",
    "\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim):\n",
    "        super(MF, self).__init__()\n",
    "\n",
    "        # user and item embeddings\n",
    "        self.user_embeddings = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embed_dim)\n",
    "\n",
    "        self.item_biases = nn.Embedding(num_items, 1)\n",
    "\n",
    "        # weight initialization\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self, init_method=\"uniform\"):\n",
    "        init = get_initializer(init_method)\n",
    "        zero_init = get_initializer(\"zeros\")\n",
    "        init(self.user_embeddings.weight)\n",
    "        init(self.item_embeddings.weight)\n",
    "        zero_init(self.item_biases.weight)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embs = self.user_embeddings(user_ids)\n",
    "        item_embs = self.item_embeddings(item_ids)\n",
    "        item_bias = self.item_biases(item_ids)\n",
    "        ratings = inner_product(user_embs, item_embs) + torch.squeeze(item_bias)\n",
    "        return ratings\n",
    "\n",
    "    def predict(self, user_ids):\n",
    "        user_ids = torch.from_numpy(np.asarray(user_ids)).long().to(self.item_embeddings.weight.device)\n",
    "        user_embs = self.user_embeddings(user_ids)\n",
    "        ratings = torch.matmul(user_embs, self.item_embeddings.weight.T)\n",
    "        ratings += torch.squeeze(self.item_biases.weight)\n",
    "        return ratings.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-01 07:33:48.520: metrics:\tPrecision@10\tPrecision@20\tRecall@10   \tRecall@20   \tMAP@10      \tMAP@20      \tNDCG@10     \tNDCG@20     \tMRR@10      \tMRR@20      \n",
      "2020-11-01 07:33:51.900: epoch 0:\t[0.2706257  0.22693552 0.10653991 0.16577937 0.17681025 0.14494169\n",
      " 0.29247102 0.27834007 0.5023895  0.5068638 ]\n",
      "2020-11-01 07:33:55.401: epoch 1:\t[0.30657503 0.24342546 0.12755449 0.18569347 0.22068861 0.17701194\n",
      " 0.34513623 0.319185   0.59120387 0.59507126]\n",
      "2020-11-01 07:33:59.391: epoch 2:\t[0.30848414 0.25206807 0.13225918 0.19953209 0.2256656  0.18437687\n",
      " 0.35220337 0.3322045  0.6052692  0.6096919 ]\n",
      "2020-11-01 07:34:02.992: epoch 3:\t[0.32322413 0.26309663 0.14436176 0.21571401 0.23419845 0.19284433\n",
      " 0.36425504 0.34561378 0.60613894 0.60995245]\n",
      "2020-11-01 07:34:06.611: epoch 4:\t[0.33128342 0.2730648  0.15050459 0.2312524  0.24044165 0.20215398\n",
      " 0.37315232 0.35920298 0.61743766 0.6215096 ]\n",
      "2020-11-01 07:34:10.331: epoch 5:\t[0.34443277 0.2829269  0.15992226 0.24208647 0.25061163 0.21159233\n",
      " 0.38434368 0.37016654 0.6158559  0.61999375]\n",
      "2020-11-01 07:34:13.597: epoch 6:\t[0.34825033 0.29035002 0.16131763 0.25100064 0.25515798 0.21782975\n",
      " 0.38888356 0.3784525  0.6224032  0.62635857]\n",
      "2020-11-01 07:34:16.584: epoch 7:\t[0.35397694 0.29443267 0.16330291 0.2561458  0.26211476 0.2236761\n",
      " 0.39653033 0.38558984 0.6352665  0.6397024 ]\n",
      "2020-11-01 07:34:19.683: epoch 8:\t[0.3604456  0.29989403 0.16535239 0.26045123 0.2669568  0.22880672\n",
      " 0.4019238  0.39118797 0.6403136  0.6440116 ]\n",
      "2020-11-01 07:34:22.990: epoch 9:\t[0.3673386  0.30307525 0.16821046 0.26313746 0.27205193 0.23271835\n",
      " 0.40816692 0.39540705 0.644998   0.64823335]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from util.pytorch import pairwise_loss\n",
    "from util.common import Reduction\n",
    "from util.pytorch import l2_loss\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mf = MF(dataset.num_users, dataset.num_items, config[\"embedding_size\"]).to(device)\n",
    "mf.reset_parameters(config[\"param_init\"])\n",
    "optimizer = torch.optim.Adam(mf.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "logger.info(evaluator.metrics_info())  # show the metrics information\n",
    "for epoch in range(10):\n",
    "    mf.train()\n",
    "    for bat_users, bat_pos_items, bat_neg_items in data_iter:\n",
    "        bat_users = torch.from_numpy(bat_users).long().to(device)\n",
    "        bat_pos_items = torch.from_numpy(bat_pos_items).long().to(device)\n",
    "        bat_neg_items = torch.from_numpy(bat_neg_items).long().to(device)\n",
    "        yui = mf(bat_users, bat_pos_items)\n",
    "        yuj = mf(bat_users, bat_neg_items)\n",
    "\n",
    "        loss = pairwise_loss(\"bpr\", yui-yuj, reduction=Reduction.SUM)\n",
    "        reg_loss = l2_loss(mf.user_embeddings(bat_users),\n",
    "                           mf.item_embeddings(bat_pos_items),\n",
    "                           mf.item_embeddings(bat_neg_items),\n",
    "                           mf.item_biases(bat_pos_items),\n",
    "                           mf.item_biases(bat_neg_items))\n",
    "        loss += config[\"reg\"] * reg_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    mf.eval()\n",
    "    result = evaluator.evaluate(mf)\n",
    "    logger.info(\"epoch %d:\\t%s\" % (epoch, result))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\NeuRec\\util\\tensorflow\\func.py:18: The name tf.initializers.he_normal is deprecated. Please use tf.compat.v1.initializers.he_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Projects\\NeuRec\\util\\tensorflow\\func.py:19: The name tf.initializers.he_uniform is deprecated. Please use tf.compat.v1.initializers.he_uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\pavel\\.conda\\envs\\NeuRec3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "from util.tensorflow import inner_product, l2_loss\n",
    "from util.tensorflow import pairwise_loss\n",
    "from util.tensorflow import get_initializer, get_session\n",
    "\n",
    "\n",
    "class MF(object):\n",
    "    def __init__(self, config, num_users, num_items):\n",
    "        self.emb_size = config[\"embedding_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.reg = config[\"reg\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.param_init = config[\"param_init\"]\n",
    "        self.loss_func = config[\"loss_func\"]\n",
    "\n",
    "        self.num_users, self.num_items = num_users, num_items\n",
    "\n",
    "        self._build_model()\n",
    "        self.sess = get_session(config[\"gpu_mem\"])\n",
    "\n",
    "    def _create_variable(self):\n",
    "        self.user_ph = tf.placeholder(tf.int32, [None], name=\"user\")\n",
    "        self.pos_item_ph = tf.placeholder(tf.int32, [None], name=\"pos_item\")\n",
    "        self.neg_item_ph = tf.placeholder(tf.int32, [None], name=\"neg_item\")\n",
    "        self.label_ph = tf.placeholder(tf.float32, [None], name=\"label\")\n",
    "\n",
    "        # embedding layers\n",
    "        init = get_initializer(self.param_init)\n",
    "        zero_init = get_initializer(\"zeros\")\n",
    "        self.user_embeddings = tf.Variable(init([self.num_users, self.emb_size]),\n",
    "                                           name=\"user_embedding\")\n",
    "        self.item_embeddings = tf.Variable(init([self.num_items, self.emb_size]),\n",
    "                                           name=\"item_embedding\")\n",
    "        self.item_biases = tf.Variable(zero_init([self.num_items]), name=\"item_bias\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        self._create_variable()\n",
    "        user_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)\n",
    "        pos_item_emb = tf.nn.embedding_lookup(self.item_embeddings, self.pos_item_ph)\n",
    "        neg_item_emb = tf.nn.embedding_lookup(self.item_embeddings, self.neg_item_ph)\n",
    "        pos_bias = tf.gather(self.item_biases, self.pos_item_ph)\n",
    "        neg_bias = tf.gather(self.item_biases, self.neg_item_ph)\n",
    "\n",
    "        yi_hat = inner_product(user_emb, pos_item_emb) + pos_bias\n",
    "        yj_hat = inner_product(user_emb, neg_item_emb) + neg_bias\n",
    "\n",
    "        # reg loss\n",
    "        model_loss = pairwise_loss(\"bpr\", yi_hat-yj_hat, reduction=Reduction.SUM)\n",
    "        reg_loss = l2_loss(user_emb, pos_item_emb, pos_bias, neg_item_emb, neg_bias)\n",
    "\n",
    "        final_loss = model_loss + self.reg * reg_loss\n",
    "\n",
    "        self.train_opt = tf.train.AdamOptimizer(self.lr).minimize(final_loss, name=\"train_opt\")\n",
    "\n",
    "        # for evaluation\n",
    "        u_emb = tf.nn.embedding_lookup(self.user_embeddings, self.user_ph)\n",
    "        self.batch_ratings = tf.matmul(u_emb, self.item_embeddings, transpose_b=True) + self.item_biases\n",
    "\n",
    "    def predict(self, users):\n",
    "        all_ratings = self.sess.run(self.batch_ratings, feed_dict={self.user_ph: users})\n",
    "        return all_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\NeuRec\\util\\tensorflow\\loss.py:60: The name tf.log_sigmoid is deprecated. Please use tf.math.log_sigmoid instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Projects\\NeuRec\\util\\tensorflow\\func.py:46: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Projects\\NeuRec\\util\\tensorflow\\func.py:50: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Projects\\NeuRec\\util\\tensorflow\\func.py:51: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "2020-11-01 07:34:26.414: metrics:\tPrecision@10\tPrecision@20\tRecall@10   \tRecall@20   \tMAP@10      \tMAP@20      \tNDCG@10     \tNDCG@20     \tMRR@10      \tMRR@20      \n",
      "2020-11-01 07:34:27.148: epoch 0:\t[0.27158022 0.22428443 0.10478995 0.16944468 0.17851605 0.14222392\n",
      " 0.29471102 0.27841341 0.50808597 0.5150114 ]\n",
      "2020-11-01 07:34:27.703: epoch 1:\t[0.30307555 0.24199395 0.12419485 0.18160151 0.2174702  0.1712266\n",
      " 0.34009495 0.3125133  0.58186805 0.5860953 ]\n",
      "2020-11-01 07:34:28.243: epoch 2:\t[0.31134722 0.25238633 0.13338241 0.1981002  0.22224434 0.18077408\n",
      " 0.3500989  0.32830843 0.596429   0.6000306 ]\n",
      "2020-11-01 07:34:28.780: epoch 3:\t[0.32640558 0.26850498 0.14463207 0.2192234  0.23716715 0.19587216\n",
      " 0.36720735 0.34935266 0.6100146  0.61332285]\n",
      "2020-11-01 07:34:29.407: epoch 4:\t[0.33647993 0.2791623  0.15279293 0.23522587 0.24489653 0.20680861\n",
      " 0.37916517 0.36510718 0.62650234 0.6301818 ]\n",
      "2020-11-01 07:34:29.928: epoch 5:\t[0.34899297 0.2864794  0.16192959 0.24572062 0.25415975 0.21437074\n",
      " 0.3892838  0.37427926 0.6245045  0.6281069 ]\n",
      "2020-11-01 07:34:30.464: epoch 6:\t[0.35641593 0.29528093 0.16515829 0.25688314 0.26350993 0.22428836\n",
      " 0.3994235  0.38716725 0.64130366 0.64501643]\n",
      "2020-11-01 07:34:30.996: epoch 7:\t[0.3618242  0.3009543  0.16920805 0.2609713  0.2675417  0.23006038\n",
      " 0.40348956 0.39209968 0.63737935 0.6403708 ]\n",
      "2020-11-01 07:34:31.533: epoch 8:\t[0.3642632  0.3022268  0.1699168  0.26476187 0.2707083  0.23145576\n",
      " 0.40645713 0.39484245 0.6413408  0.645155  ]\n",
      "2020-11-01 07:34:32.080: epoch 9:\t[0.36415708 0.30625665 0.16939667 0.26747957 0.270254   0.23391482\n",
      " 0.40653843 0.39804664 0.6450186  0.64883   ]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_users, num_items = dataset.num_users, dataset.num_items\n",
    "mf = MF(config, num_users, num_items)\n",
    "logger.info(evaluator.metrics_info())\n",
    "for epoch in range(10):\n",
    "    for bat_users, bat_pos_items, bat_neg_items in data_iter:\n",
    "        feed = {mf.user_ph: bat_users,\n",
    "                mf.pos_item_ph: bat_pos_items,\n",
    "                mf.neg_item_ph: bat_neg_items}\n",
    "        mf.sess.run(mf.train_opt, feed_dict=feed)\n",
    "    result = evaluator.evaluate(mf)\n",
    "    logger.info(\"epoch %d:\\t%s\" % (epoch, result))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
